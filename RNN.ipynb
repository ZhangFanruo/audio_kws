{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword speech recongnition\n",
    "\n",
    "The idea is to use Convolution Neural Network to extract features of the framed specturm and classify the audio data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.contrib.learn.python.learn.learn_io.generator_io import generator_input_fn\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from scipy import signal\n",
    "\n",
    "import h5py\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/audio_sample' # path to training data\n",
    "TEMP_DATA_PATH = '/tmp/temp'\n",
    "TRAIN_DATA_PATH = DATA_PATH+ '/audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LICENSE',\n",
       " 'validation_list.txt',\n",
       " 'audio',\n",
       " 'README.md',\n",
       " 'testing_list.txt',\n",
       " '.DS_Store']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero', 'right', 'off', 'seven', 'nine', 'dog', 'left', 'five', 'bird', 'two', 'three', 'one', 'happy', 'bed', '_background_noise_', 'on', 'house', 'six', 'yes', 'up', 'marvin', 'cat', 'wow', 'go', 'no', 'tree', 'sheila', 'down', 'eight', 'four', 'stop']\n",
      "There are totally 31 labels in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "folders = []\n",
    "for file_folder in os.listdir(TRAIN_DATA_PATH):\n",
    "    if os.path.isdir(TRAIN_DATA_PATH + file_folder):\n",
    "        folders.append(file_folder)\n",
    "print(folders)\n",
    "print('There are totally ' + str(len(folders)) + ' labels in the training dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, from the competition note, there are only 12 labels we have to pay attention to.\n",
    "> **Note**: There are only 12 possible labels for the Test set: yes, no, up, down, left, right, on, off, stop, go, silence, unknown.\n",
    "\n",
    "> The unknown label should be used for a command that is not one one of the first 10 labels or that is not silence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "POSSIBLE_LABELS = 'yes, no, up, down, left, right, on, off, stop, go, silence, unknown'.replace(' ', '').split(',')\n",
    "print(POSSIBLE_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The file *validation_list.txt* specifies the data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in validation files list and test files list\n",
    "VALIDATION_LIST_FILE_PATH = DATA_PATH + '/validation_list.txt'\n",
    "with open(VALIDATION_LIST_FILE_PATH, 'r') as file:\n",
    "    VALIDATION_FILE_NAMES = [line.rstrip() for line in file]\n",
    "\n",
    "TEST_LIST_FILE_PATH = DATA_PATH + '/testing_list.txt'\n",
    "with open(TEST_LIST_FILE_PATH, 'r') as file:\n",
    "    TEST_FILE_NAMES = [line.rstrip() for line in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matcher = re.compile(\"(?:.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "def get_info_from_path(path):\n",
    "    \"\"\"\n",
    "        return (label, id)\n",
    "    \"\"\"\n",
    "    r = sample_matcher.match(path)\n",
    "    if not r:\n",
    "        raise ValueError(path + ' is not valid file path')\n",
    "    label = r.group(1)\n",
    "    id = r.group(2)\n",
    "    \n",
    "    if label not in POSSIBLE_LABELS:\n",
    "        if label == '_background_noise_':\n",
    "            label = 'silence'\n",
    "        else:\n",
    "            label = 'unknown'\n",
    "    return label, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract info for validation and test files\n",
    "VALIDATION_FILES = []\n",
    "VALIDATIN_ID_SET = {}\n",
    "for file in VALIDATION_FILE_NAMES:\n",
    "    try: \n",
    "        label, id = get_info_from_path(file)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    VALIDATION_FILES.append((label, id, file))\n",
    "    VALIDATIN_ID_SET[id] = True\n",
    "\n",
    "TEST_FILES = []\n",
    "TEST_ID_SET = {}\n",
    "for file in TEST_FILE_NAMES:\n",
    "    try: \n",
    "        label, id = get_info_from_path(file)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    TEST_FILES.append((label, id, file))\n",
    "    TEST_ID_SET[id] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train file is the file that neither in test or validation\n",
    "file_path_matcher = re.compile(\"(.+\\/)?(\\w+\\/[^_]+_.+wav)\")\n",
    "TRAIN_FILES = []\n",
    "SAMPLE_FILES = glob(os.path.join(DATA_PATH, 'audio/*/*wav'))\n",
    "for file in SAMPLE_FILES:\n",
    "    try:\n",
    "        label, id = get_info_from_path(file)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if (id not in VALIDATIN_ID_SET) and (id not in TEST_ID_SET):\n",
    "        TRAIN_FILES.append((label, id, file_path_matcher.match(file).group(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sample\n",
      "Total: 64727\n",
      "Train: 51094\n",
      "Validation: 6798\n",
      "Test: 6835\n",
      "51094 + 6798 + 6835 = 64727\n"
     ]
    }
   ],
   "source": [
    "print('Number of sample')\n",
    "print('Total: %d' % len(SAMPLE_FILES))\n",
    "print('Train: %d' % len(TRAIN_FILES))\n",
    "print('Validation: %d' % len(VALIDATION_FILES))\n",
    "print('Test: %d' % len(TEST_FILES))\n",
    "number_sum = len(TRAIN_FILES) + len(VALIDATION_FILES) + len(TEST_FILES)\n",
    "print('{} + {} + {} = {}'.format(len(TRAIN_FILES), len(VALIDATION_FILES), len(TEST_FILES), number_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate the distribution of training and validation data\n",
    "distribution = {}\n",
    "for label, _, _ in TRAIN_FILES + VALIDATION_FILES:\n",
    "    if label not in distribution:\n",
    "        distribution[label] = 0\n",
    "    distribution[label] = distribution[label] + 1\n",
    "\n",
    "data = [go.Bar(\n",
    "            x=[*distribution.keys()],\n",
    "            y=[*distribution.values()]\n",
    "    )]\n",
    "\n",
    "iplot(data, filename='distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the distribution of the data, **unknwon**, **silence** are two special labels. **unknown** has much more data than others, **silence** only has 6.\n",
    "\n",
    "\n",
    "Ideas to handle the two special cases:\n",
    "- train a model to seperate **silence** from the rest. Then we train another one to seperate **unknown** from the key words that we are interesting.\n",
    "\n",
    "- bootstrap the other types\n",
    "\n",
    "- throw away some of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution = {}\n",
    "for label, _, _ in VALIDATION_FILES:\n",
    "    if label not in distribution:\n",
    "        distribution[label] = 0\n",
    "    distribution[label] = distribution[label] + 1\n",
    "\n",
    "data = [go.Bar(\n",
    "            x=[*distribution.keys()],\n",
    "            y=[*distribution.values()]\n",
    "    )]\n",
    "\n",
    "iplot(data, filename='distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wierd thing is that validation set does not have **silence**. So in order to train the silence detect model, I have to use the train set for both training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. audio length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_length_list = []\n",
    "\n",
    "# read in every audio and count each length\n",
    "for label, id, file in TRAIN_FILES + VALIDATION_FILES:\n",
    "    sample_rate, samples = wavfile.read(TRAIN_DATA_PATH + file)\n",
    "    sample_length_list.append(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "sample_length_list = np.array(sample_length_list)\n",
    "sample_length_list[sample_length_list > sample_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_valid_files = np.array(TRAIN_FILES + VALIDATION_FILES)\n",
    "large_audio_files = train_valid_files[sample_length_list > sample_rate]\n",
    "print(large_audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some really long audio. Hopefully, there are not too much and they are those background noise. Let's hear some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ipd.Audio(filename=TRAIN_DATA_PATH + large_audio_files[4][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some ideas to handle it:**\n",
    "- chop the long file into 1 second length\n",
    "- mix different types of noise\n",
    "- mix noise with audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "sample_length_list = np.array(sample_length_list)\n",
    "sample_length_list[sample_length_list < 0.5 * sample_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some very short audio. Also not too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "truncated_sample_length = sample_length_list[sample_length_list <= 3 * sample_rate]\n",
    "data = [go.Histogram(x=truncated_sample_length)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_sample_num = len(sample_length_list[sample_length_list < sample_rate])\n",
    "print('There are about %d samples smaller than the sample rate' % small_sample_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some idea to handle short audio sample:**\n",
    "- padding 0 to it\n",
    "- stretch the audio to 1 seconds\n",
    "- decompose it into phonome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_middle(sample, sample_rate=16000):\n",
    "    pad_num = sample_rate - len(sample)\n",
    "    left = int(pad_num / 2)\n",
    "    right = pad_num - left\n",
    "    return np.pad(sample, (left, right), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_data(sample, sample_rate=16000):\n",
    "    \"\"\"\n",
    "        chop the data by cutting it into small parts first\n",
    "    \"\"\"\n",
    "    num = np.ceil(len(sample) / sample_rate).astype(np.int)\n",
    "    pad_num = num * sample_rate - len(sample)\n",
    "    return np.split(np.pad(sample, (0, pad_num), 'constant'), num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select(sample, sample_rate=16000):\n",
    "    beg = np.random.randint(0, len(sample) - sample_rate)\n",
    "    return sample[beg: beg + sample_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(label):\n",
    "    if label == 'silence':\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning:\n",
      "\n",
      "Chunk (non-data) not understood, skipping it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_list = []\n",
    "\n",
    "# read file into memory\n",
    "if len(train_data_list) < 1:\n",
    "    for label, id, fname in TRAIN_FILES:\n",
    "        sample_rate, sample = wavfile.read(TRAIN_DATA_PATH + fname)\n",
    "        train_data_list.append((label, id, sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the silence samples\n",
    "silence_samples = list(filter(lambda t: t[0] == 'silence', train_data_list))\n",
    "non_silence_samples = list(filter(lambda t: t[0] != 'silence', train_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir for sliced silence samples\n",
    "SLICED_SILENCE_TRAIN_FILES_PATH = os.path.join(TEMP_DATA_PATH, 'augumented_data/short_silence_train')\n",
    "SLICED_SILENCE_VALID_FILES_PATH = os.path.join(TEMP_DATA_PATH, 'augumented_data/short_silence_valid')\n",
    "\n",
    "os.makedirs(SLICED_SILENCE_TRAIN_FILES_PATH, exist_ok=True)\n",
    "os.makedirs(SLICED_SILENCE_VALID_FILES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output silence sample from 80% of the noise sample for trainning set\n",
    "if True:\n",
    "    for i in range(len(non_silence_samples)):\n",
    "        index = np.random.randint(0, len(silence_samples))\n",
    "        if len(silence_samples[index][2]) < 16000:\n",
    "            print(silence_samples[index][1])\n",
    "        \n",
    "        end = int(len(silence_samples[index][2]) * 0.8)\n",
    "        random_sample = random_select(silence_samples[index][2][:end], 16000)\n",
    "        title = silence_samples[index][0]\n",
    "        file_name = title + '_' + str(index) + '_' + str(i) + '.wav'\n",
    "        wavfile.write(os.path.join(SLICED_SILENCE_TRAIN_FILES_PATH, file_name), 16000, random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output silence sample from 20% of the noise sample for validation set\n",
    "if True:\n",
    "    for i in range(len(non_silence_samples)):\n",
    "        index = np.random.randint(0, len(silence_samples))\n",
    "        if len(silence_samples[index][2]) < 16000:\n",
    "            print(silence_samples[i])\n",
    "        beg = int(len(silence_samples[index][2]) * 0.8)\n",
    "        random_sample = random_select(silence_samples[index][2][beg:], 16000)\n",
    "        title = silence_samples[index][0]\n",
    "        file_name = title + '_' + str(index) + '_' + str(i) + '.wav'\n",
    "        wavfile.write(os.path.join(SLICED_SILENCE_VALID_FILES_PATH, file_name), 16000, random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally we have 51088 silence training data\n",
      "Totally we have 51088 silence valid data\n"
     ]
    }
   ],
   "source": [
    "# load the silence data for training \n",
    "silence_data_train =[]\n",
    "\n",
    "for file in glob(os.path.join(SLICED_SILENCE_TRAIN_FILES_PATH, '*wav')):\n",
    "    _, sample = wavfile.read(file)\n",
    "    r = sample_matcher.match(file)\n",
    "    silence_data_train.append(('silence', r.group(2), sample))\n",
    "    \n",
    "# load the silence data for validation\n",
    "\n",
    "silence_data_valid =[]\n",
    "\n",
    "for file in glob(os.path.join(SLICED_SILENCE_VALID_FILES_PATH, '*wav')):\n",
    "    _, sample = wavfile.read(file)\n",
    "    r = sample_matcher.match(file)\n",
    "    silence_data_valid.append(('silence', r.group(2), sample))\n",
    "    \n",
    "# print count\n",
    "\n",
    "print('Totally we have %d silence training data' % len(silence_data_train))\n",
    "print('Totally we have %d silence valid data' % len(silence_data_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_list = []\n",
    "\n",
    "if len(valid_data_list) < 1:\n",
    "    for label, id, fname in VALIDATION_FILES:\n",
    "        sample_rate, sample = wavfile.read(TRAIN_DATA_PATH + fname)\n",
    "        valid_data_list.append((label, id, sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TEST_FILE_PATH = '/audio_test/test/audio'\n",
    "FINAL_TEST_FILES = glob(os.path.join(FINAL_TEST_FILE_PATH, '*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_raw = []\n",
    "for file_path in FINAL_TEST_FILES:\n",
    "    uid = test_file_id_matcher.search(file_path).group(1)\n",
    "    sample_rate, sample = wavfile.read(file_path)\n",
    "    test_samples_raw.append((uid, sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. CNN with spectrum\n",
    "The idea is to chop the audio data into different frame. Take the frequency info from each frame and put those into CNN. As mentioned before, the plan is to build 3 models: detect silence, detect unkown, classify the data. And combine them finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. the first model is to detect the silence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF Estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there could be some more advanced way to mixup\n",
    "silence_model_train_data = non_silence_samples + silence_data_train\n",
    "\n",
    "# mixup can be used here\n",
    "silence_model_valid_data = valid_data_list + silence_data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silence_train_input_genetator(data_list=silence_model_train_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    To train the model to detect silence.\n",
    "    For silence data we have to find some\n",
    "    way to geneate some in order to handle\n",
    "    the misbalance problem.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.shuffle(silence_model_train_data)\n",
    "    def generator():\n",
    "        for label, id, sample in data_list:\n",
    "            try:\n",
    "                sample = sample.astype(np.float32) / np.iinfo(np.int16).max\n",
    "                if len(sample) < sample_rate:\n",
    "                    samples = [pad_to_middle(sample)]\n",
    "                elif len(sample) > sample_rate:\n",
    "                    samples = chop_data(sample)\n",
    "                else:\n",
    "                    samples = [sample]\n",
    "                for one_second_sample in samples:\n",
    "                    yield dict(\n",
    "                        target=np.int32(get_label(label)),\n",
    "                        wav=one_second_sample\n",
    "                    )\n",
    "            except Exception as err:\n",
    "                print(err, label, id, fname)\n",
    "\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silence_eval_input_genetator(data_list=silence_model_valid_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    To train the model to detect silence.\n",
    "    For silence data we have to find some\n",
    "    way to geneate some in order to handle\n",
    "    the misbalance problem.\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        for label, id, sample in data_list:\n",
    "            try:\n",
    "                sample = sample.astype(np.float32) / np.iinfo(np.int16).max\n",
    "                if len(sample) < sample_rate:\n",
    "                    samples = [pad_to_middle(sample)]\n",
    "                elif len(sample) > sample_rate:\n",
    "                    samples = [chop_data(sample)[0]]\n",
    "                else:\n",
    "                    samples = [sample]\n",
    "                for one_second_sample in samples:\n",
    "                    yield dict(\n",
    "                        target=np.int32(get_label(label)),\n",
    "                        wav=one_second_sample\n",
    "                    )\n",
    "            except Exception as err:\n",
    "                print(err, label, id, fname)\n",
    "\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers, signal\n",
    "def silence_model_handler(features, labels, mode, params, config):\n",
    "    \n",
    "    ### ================ Define the CNN ==========================\n",
    "    sample = features['wav']\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    x = tf.abs(signal.stft(sample, 400, 160))\n",
    "    x = tf.stack([x], axis=3)\n",
    "    x = tf.to_float(x)\n",
    "    x = layers.batch_norm(x, is_training=is_training)\n",
    "    for i in range(3):\n",
    "        x = layers.conv2d(\n",
    "            x, 8 * (2 ** i), 3, 1,\n",
    "            normalizer_fn=layers.batch_norm,\n",
    "            normalizer_params={'is_training': is_training}\n",
    "        )\n",
    "        x = layers.max_pool2d(x, 2, 2)\n",
    "    x = layers.flatten(x)\n",
    "    x = tf.layers.dense(x, 128, activation=tf.nn.relu)\n",
    "    x = layers.dropout(x, keep_prob=params['keep_prob'] if is_training else 1.0)\n",
    "    x = tf.layers.dense(x, 64, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(x, 2, activation=None)\n",
    "\n",
    "    ### ===========================================================\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "        train_op=layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            learning_rate=0.001,\n",
    "            optimizer=tf.train.AdamOptimizer\n",
    "        )\n",
    "        \n",
    "        specs = {'mode': mode, 'loss': loss, 'train_op': train_op}\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        prediction = tf.argmax(logits, axis=-1)\n",
    "        acc, acc_op = tf.metrics.mean_per_class_accuracy(\n",
    "            labels, prediction, 2)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "        specs = dict(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=dict(\n",
    "                    acc=(acc, acc_op),\n",
    "                )\n",
    "        )\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'label': tf.argmax(logits, axis=-1),  \n",
    "            'sample': features['wav'], \n",
    "        }\n",
    "        specs = {\n",
    "            'mode': mode,\n",
    "            'predictions': predictions,\n",
    "        }\n",
    "    return tf.estimator.EstimatorSpec(**specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_PATH = os.path.join('./', 'out')\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "params = {\n",
    "    'keep_prob': True\n",
    "}\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    model_dir=OUT_PATH, \n",
    "    log_step_count_steps=10,\n",
    "    save_summary_steps=10\n",
    ")\n",
    "silence_model = tf.estimator.Estimator(\n",
    "    model_fn=silence_model_handler, params=params, config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# silence_model_input_fn = silence_train_input_from_memory(silence_model_train_data)\n",
    "silence_model_input_fn = generator_input_fn(\n",
    "    x=silence_train_input_genetator(silence_model_train_data),\n",
    "    target_key='target',\n",
    "    shuffle=True,\n",
    "    num_epochs=None,\n",
    "    batch_size=50,\n",
    "    queue_capacity=3 * 50 + 10, num_threads=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_input = lambda: silence_train_input_from_memory(data_list)\n",
    "silence_model.train(input_fn=silence_model_input_fn, steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_model_eval_input_fn = generator_input_fn(\n",
    "    x=silence_eval_input_genetator(silence_model_valid_data),\n",
    "    target_key='target'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_model.evaluate(input_fn=silence_model_eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Estimator from tensorflow is not very developer friendly, hard to learn and also need a lot of redundancy codes. So I switch to keras which is very friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate=16000, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = scipy.signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_onehot_spectrum_transform_silence(samples):\n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    x = list(map(lambda t: t[2], samples))\n",
    "    y = list(map(lambda t: 1 if t[0] == 'silence' else 0, samples))\n",
    "    y = keras.utils.to_categorical(y, num_classes=2)\n",
    "    \n",
    "    x = list(map(pad_to_middle, x))\n",
    "    spectrum = np.stack(map(log_specgram, x))\n",
    "    spectrum = np.array(spectrum)\n",
    "    spectrum = spectrum.reshape(-1, 99, 161, 1)\n",
    "    \n",
    "    return spectrum, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_model_train_data_x_spectrum, silence_model_train_data_y = shuffle_onehot_spectrum_transform_silence(silence_model_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_model_valid_data_x_spectrum, silence_model_valid_data_y = shuffle_onehot_spectrum_transform_silence(silence_model_valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_model_h5py = h5py.File('silence_model_data')\n",
    "silence_model_h5py.create_dataset('silence_model_train_data_x_spectrum', data=silence_model_train_data_x_spectrum)\n",
    "silence_model_h5py.create_dataset('silence_model_train_data_y', data=silence_model_train_data_y)\n",
    "silence_model_h5py.create_dataset('silence_model_valid_data_x_spectrum', data=silence_model_valid_data_x_spectrum)\n",
    "silence_model_h5py.create_dataset('silence_model_valid_data_y', data=silence_model_valid_data_y)\n",
    "silence_model_h5py.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vgg model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers_1(model):\n",
    "    model.add(BatchNormalization(input_shape=(99, 161,1)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "silence_model = Sequential()\n",
    "vgg_layers_1(silence_model)\n",
    "silence_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "silence_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join('./', 'silence-checkpoint-{epoch:02d}-{acc:.2f}.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102176 samples, validate on 57886 samples\n",
      "Epoch 1/5\n",
      "102176/102176 [==============================] - 120s - loss: 0.1248 - acc: 0.9549 - val_loss: 0.0577 - val_acc: 0.9804\n",
      "Epoch 2/5\n",
      "102176/102176 [==============================] - 112s - loss: 0.0259 - acc: 0.9917 - val_loss: 0.2255 - val_acc: 0.9233\n",
      "Epoch 3/5\n",
      "102176/102176 [==============================] - 112s - loss: 0.0126 - acc: 0.9961 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "Epoch 4/5\n",
      "102176/102176 [==============================] - 113s - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "Epoch 5/5\n",
      "102176/102176 [==============================] - 112s - loss: 0.0015 - acc: 0.9996 - val_loss: 5.2960e-04 - val_acc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb726298668>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silence_model.fit(x=silence_model_train_data_x_spectrum, y=silence_model_train_data_y, \n",
    "                  batch_size=32, epochs=5, callbacks=[checkpoint],\n",
    "                  validation_data=(silence_model_valid_data_x_spectrum, silence_model_valid_data_y)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_model.save('silence_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vgg model above is not suitable for audio recongnition (it is designed for computer vision). So I also tried the following conv net from [google tensorflow speech command](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vgg model 2 from google speech command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_google(model):\n",
    "    model.add(BatchNormalization(input_shape=(99, 161,1)))\n",
    "    model.add(Conv2D(64, (8, 20), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (4, 10), activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "silence_model_google = Sequential()\n",
    "vgg_google(silence_model_google)\n",
    "silence_model_google.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "silence_model_google.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102176 samples, validate on 57886 samples\n",
      "Epoch 1/5\n",
      "102176/102176 [==============================] - 93s - loss: 0.0261 - acc: 0.9912 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "102176/102176 [==============================] - 92s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "102176/102176 [==============================] - 92s - loss: 9.6080e-04 - acc: 0.9998 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "102176/102176 [==============================] - 92s - loss: 9.6655e-04 - acc: 0.9996 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "102176/102176 [==============================] - 92s - loss: 1.8881e-04 - acc: 1.0000 - val_loss: 6.9612e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7202a5908>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join('./', 'silence-google-checkpoint-{epoch:02d}-{acc:.2f}.hdf5'))\n",
    "silence_model_google.fit(x=silence_model_train_data_x_spectrum, y=silence_model_train_data_y, \n",
    "                  batch_size=32, epochs=5, callbacks=[checkpoint],\n",
    "                  validation_data=(silence_model_valid_data_x_spectrum, silence_model_valid_data_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_model_google.save('silence_model_google')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is very high. But consider the validation, training set comes from the same source. The model may overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mixed silence with samples model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix up the silence and audio\n",
    "(label, 0.8 * audio + 0.1 * noise1 + 0.1 * noise2) vs (silence, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(audio, noises):\n",
    "    noise_index_1 = np.random.randint(len(noises))\n",
    "    noise_index_2 = np.random.randint(len(noises))\n",
    "    if len(audio) < 16000:\n",
    "        audio = pad_to_middle(audio)\n",
    "    mixed_audio = 0.8 * audio + 0.1 * random_select(noises[noise_index_1]) + 0.1 * random_select(noises[noise_index_2])\n",
    "    return mixed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXUP_SILENCE_TRAIN_PATH = os.path.join(TEMP_DATA_PATH ,'augumented_data/mixup_silence_train')\n",
    "MIXUP_SILENCE_VALID_PATH = os.path.join(TEMP_DATA_PATH, 'augumented_data/mixup_silence_valid')\n",
    "\n",
    "os.makedirs(MIXUP_SILENCE_TRAIN_PATH, exist_ok=True)\n",
    "os.makedirs(MIXUP_SILENCE_VALID_PATH, exist_ok=True)\n",
    "\n",
    "silence_train_samples = list(map(lambda t: t[2][:int(0.8 * len(t[2]))], silence_samples))\n",
    "silence_valid_samples = list(map(lambda t: t[2][int(0.8 * len(t[2])):], silence_samples))\n",
    "\n",
    "\n",
    "if True:\n",
    "    for _ in range(int(0.3 * len(non_silence_samples))):\n",
    "        index = np.random.randint(len(non_silence_samples))\n",
    "        label, uid, sample = non_silence_samples[index]\n",
    "        mixed_sample = mixup(sample, silence_train_samples)\n",
    "        file_name = 'label' + '_mixed_' + str(index) + '_' + str(uid) + '.wav'\n",
    "        wavfile.write(os.path.join(MIXUP_SILENCE_TRAIN_PATH, file_name), 16000, mixed_sample)\n",
    "\n",
    "if True:\n",
    "    for _ in range(int(0.3 * len(valid_data_list))):\n",
    "        index = np.random.randint(len(valid_data_list))\n",
    "        label, uid, sample = valid_data_list[index]\n",
    "        mixed_sample = mixup(sample, silence_train_samples)\n",
    "        file_name = 'label' + '_mixed_' + str(index) + '_' + str(uid) + '.wav'\n",
    "        wavfile.write(os.path.join(MIXUP_SILENCE_VALID_PATH, file_name), 16000, mixed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path(path):\n",
    "    files = glob(os.path.join(path, '*.wav'))\n",
    "    samples = []\n",
    "    for file in files:\n",
    "        sample_rate, sample = wavfile.read(file)\n",
    "        label, id = get_info_from_path(file)\n",
    "        samples.append((label, id, sample))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed silence train number 13225\n",
      "Mixed silence valid number 1763\n"
     ]
    }
   ],
   "source": [
    "mixed_train_samples_no_silence = load_path(MIXUP_SILENCE_TRAIN_PATH)\n",
    "mixed_valid_samples_no_silence = load_path(MIXUP_SILENCE_VALID_PATH)\n",
    "np.random.shuffle(mixed_train_samples_no_silence)\n",
    "np.random.shuffle(mixed_valid_samples_no_silence)\n",
    "print('Mixed silence train number %d' % len(mixed_train_samples_no_silence))\n",
    "print('Mixed silence valid number %d' % len(mixed_valid_samples_no_silence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(silence_data_train)\n",
    "np.random.shuffle(silence_data_valid)\n",
    "\n",
    "\n",
    "mixed_train_samples = mixed_train_samples_no_silence + \\\n",
    "                        silence_data_train[:len(mixed_train_samples_no_silence)] + \\\n",
    "                        non_silence_samples\n",
    "mixed_valid_samples = mixed_valid_samples_no_silence + \\\n",
    "                        silence_data_valid[:len(mixed_valid_samples_no_silence)] + \\\n",
    "                        valid_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_train_x_spectrum, mixed_train_data_y = shuffle_onehot_spectrum_transform_silence(mixed_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_valid_data_x_spectrum, mixed_valid_data_y = shuffle_onehot_spectrum_transform_silence(mixed_valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_silence_data = h5py.File('mixed_silence_data')\n",
    "mixed_silence_data.create_dataset('train_x_spectrum', data=mixed_train_x_spectrum)\n",
    "mixed_silence_data.create_dataset('train_y', data=mixed_train_data_y)\n",
    "mixed_silence_data.create_dataset('valid_x_spectrum', data=mixed_train_x_spectrum)\n",
    "mixed_silence_data.create_dataset('valid_y', data=mixed_valid_data_y)\n",
    "mixed_silence_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_silence_model = Sequential()\n",
    "vgg_layers_1(mixed_silence_model)\n",
    "mixed_silence_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "mixed_silence_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join('./', 'mixed-silence-checkpoint-{epoch:02d}-{acc:.2f}.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77538 samples, validate on 10324 samples\n",
      "Epoch 1/5\n",
      "77538/77538 [==============================] - 76s - loss: 0.2318 - acc: 0.8975 - val_loss: 0.1035 - val_acc: 0.9561\n",
      "Epoch 2/5\n",
      "77538/77538 [==============================] - 75s - loss: 0.0799 - acc: 0.9671 - val_loss: 0.1035 - val_acc: 0.9661\n",
      "Epoch 3/5\n",
      "77538/77538 [==============================] - 75s - loss: 0.0330 - acc: 0.9879 - val_loss: 0.0765 - val_acc: 0.9774\n",
      "Epoch 4/5\n",
      "77538/77538 [==============================] - 75s - loss: 0.0202 - acc: 0.9934 - val_loss: 0.0689 - val_acc: 0.9800\n",
      "Epoch 5/5\n",
      "77538/77538 [==============================] - 75s - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0853 - val_acc: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb711524d68>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_silence_model.fit(x=mixed_train_x_spectrum, y=mixed_train_data_y, \n",
    "                  batch_size=32, epochs=5, callbacks=[checkpoint],\n",
    "                  validation_data=(mixed_valid_data_x_spectrum, mixed_valid_data_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_silence_model.save('mixed_silence_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**google speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_silence_google_model = Sequential()\n",
    "vgg_google(mixed_silence_google_model)\n",
    "mixed_silence_google_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "mixed_silence_google_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join('./', 'mixed-silence-google-checkpoint-{epoch:02d}-{acc:.2f}.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77538 samples, validate on 10324 samples\n",
      "Epoch 1/5\n",
      "77538/77538 [==============================] - 62s - loss: 0.1282 - acc: 0.9443 - val_loss: 0.1266 - val_acc: 0.9575\n",
      "Epoch 2/5\n",
      "77538/77538 [==============================] - 62s - loss: 0.0542 - acc: 0.9822 - val_loss: 0.1833 - val_acc: 0.9678\n",
      "Epoch 3/5\n",
      "77538/77538 [==============================] - 62s - loss: 0.0359 - acc: 0.9888 - val_loss: 0.1683 - val_acc: 0.9713\n",
      "Epoch 4/5\n",
      "77538/77538 [==============================] - 62s - loss: 0.0267 - acc: 0.9919 - val_loss: 0.1391 - val_acc: 0.9732\n",
      "Epoch 5/5\n",
      "77538/77538 [==============================] - 62s - loss: 0.0187 - acc: 0.9945 - val_loss: 0.1689 - val_acc: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb19b59ce10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_silence_google_model.fit(x=mixed_train_x_spectrum, y=mixed_train_data_y, \n",
    "                  batch_size=32, epochs=5, callbacks=[checkpoint],\n",
    "                  validation_data=(mixed_valid_data_x_spectrum, mixed_valid_data_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_silence_google_model.save('mixed_silence_google_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** b. the second model is to detect unknown words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, validation set has enough classess for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unknown': 1, 'right': 0, 'off': 0, 'left': 0, 'on': 0, 'yes': 0, 'up': 0, 'go': 0, 'no': 0, 'down': 0, 'stop': 0}\n"
     ]
    }
   ],
   "source": [
    "# set a class label for the data, 1: unknown, 0: others \n",
    "class_dict = {}\n",
    "for label, uid, sample in non_silence_samples:\n",
    "    if label not in class_dict:\n",
    "        class_dict[label] = 1 if label == 'unknown' else 0\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_onehot_spectrum_transform(samples, class_dict=class_dict):\n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    x = list(map(lambda t: t[2], samples))\n",
    "    y = list(map(lambda t: class_dict[t[0]], samples))\n",
    "    y = keras.utils.to_categorical(y, num_classes=len(class_dict))\n",
    "    \n",
    "    x = list(map(pad_to_middle, x))\n",
    "    spectrum = np.stack(map(log_specgram, x))\n",
    "    spectrum = np.array(spectrum)\n",
    "    spectrum = spectrum.reshape(-1, 99, 161, 1)\n",
    "    \n",
    "    return spectrum, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_spectrum, train_y_one_hot = shuffle_onehot_spectrum_transform(non_silence_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_x_spectrum, valid_y_one_hot = shuffle_onehot_spectrum_transform(valid_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_model = Sequential()\n",
    "vgg_layers_1(unknown_model)\n",
    "unknown_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "unknown_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=os.path.join('out', 'checkpoint-unknown-{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
    "unknown_model.fit(train_x_spectrum, train_y_one_hot, validation_data=(valid_x_spectrum, valid_y_one_hot), \n",
    "          batch_size=32, epochs=30, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    unknown_model.save('./unknown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**google vgg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_model_google = Sequential()\n",
    "vgg_layers_1(unknown_model_google)\n",
    "unknown_model_google.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "unknown_model_google.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=os.path.join('out', 'checkpoint-unknown_model_google-{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
    "unknown_model_google.fit(train_x_spectrum, train_y_one_hot, validation_data=(valid_x_spectrum, valid_y_one_hot), \n",
    "          batch_size=32, epochs=30, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    unknown_model_google.save('./unknown_model_google')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b-2. classify all non-silence data together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unknown': 0, 'right': 1, 'off': 2, 'left': 3, 'on': 4, 'yes': 5, 'up': 6, 'go': 7, 'no': 8, 'down': 9, 'stop': 10}\n"
     ]
    }
   ],
   "source": [
    "class_dict = {}\n",
    "for label, uid, sample in non_silence_samples:\n",
    "    if label not in class_dict:\n",
    "        class_dict[label] = len(class_dict)\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_spectrum, train_y_one_hot = shuffle_onehot_spectrum_transform(non_silence_samples, class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_spectrum, valid_y_one_hot = shuffle_onehot_spectrum_transform(valid_data_list, class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vgg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_model = Sequential()\n",
    "vgg_layers_1(classes_model)\n",
    "classes_model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-4)\n",
    "classes_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51088 samples, validate on 6798 samples\n",
      "Epoch 1/5\n",
      "51088/51088 [==============================] - 50s - loss: 0.4227 - acc: 0.8686 - val_loss: 0.3126 - val_acc: 0.9113\n",
      "Epoch 2/5\n",
      "51088/51088 [==============================] - 50s - loss: 0.3057 - acc: 0.9087 - val_loss: 0.2851 - val_acc: 0.9134\n",
      "Epoch 3/5\n",
      "51088/51088 [==============================] - 50s - loss: 0.2427 - acc: 0.9303 - val_loss: 0.2441 - val_acc: 0.9329\n",
      "Epoch 4/5\n",
      "51088/51088 [==============================] - 50s - loss: 0.2050 - acc: 0.9415 - val_loss: 0.2301 - val_acc: 0.9347\n",
      "Epoch 5/5\n",
      "51088/51088 [==============================] - 50s - loss: 0.1709 - acc: 0.9516 - val_loss: 0.2258 - val_acc: 0.9432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb196f0fc50>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=os.path.join(TEMP_DATA_PATH, 'classes-model-checkpoint-unknown-{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
    "classes_model.fit(train_x_spectrum, train_y_one_hot, validation_data=(valid_x_spectrum, valid_y_one_hot), \n",
    "          batch_size=32, epochs=5, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_model.save('classes_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**google**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_silence_model_google = Sequential()\n",
    "vgg_google(classes_silence_model_google)\n",
    "classes_silence_model_google.add(Dense(11, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-5)\n",
    "classes_silence_model_google.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51088 samples, validate on 6798 samples\n",
      "Epoch 1/3\n",
      "51088/51088 [==============================] - 42s - loss: 0.6003 - acc: 0.8124 - val_loss: 0.6942 - val_acc: 0.7885\n",
      "Epoch 2/3\n",
      "51088/51088 [==============================] - 42s - loss: 0.5310 - acc: 0.8333 - val_loss: 0.6299 - val_acc: 0.8029\n",
      "Epoch 3/3\n",
      "51088/51088 [==============================] - 42s - loss: 0.4780 - acc: 0.8504 - val_loss: 0.5888 - val_acc: 0.8248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1963b9240>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=os.path.join(TEMP_DATA_PATH, 'classes_silence_model_google-checkpoint-unknown-{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
    "classes_silence_model_google.fit(train_x_spectrum, train_y_one_hot, validation_data=(valid_x_spectrum, valid_y_one_hot), \n",
    "          batch_size=32, epochs=3, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_silence_model_google.save('classes_silence_model_google_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rnn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Merge, Input, merge\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Permute,Lambda, RepeatVector\n",
    "from keras.layers.convolutional import ZeroPadding2D, AveragePooling2D, Conv2D,MaxPooling2D, Convolution1D,MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, TimeDistributed, Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time = 99\n",
    "n_freq = 161\n",
    "num_classes = 11\n",
    "\n",
    "def block(input):\n",
    "    cnn = Conv2D(128, (3, 3), padding=\"same\", activation=\"linear\", use_bias=False)(input)\n",
    "    cnn = BatchNormalization(axis=-1)(cnn)\n",
    "\n",
    "    cnn1 = Lambda(slice1, output_shape=slice1_output_shape)(cnn)\n",
    "    cnn2 = Lambda(slice2, output_shape=slice2_output_shape)(cnn)\n",
    "\n",
    "    cnn1 = Activation('linear')(cnn1)\n",
    "    cnn2 = Activation('sigmoid')(cnn2)\n",
    "\n",
    "    out = Multiply()([cnn1, cnn2])\n",
    "    return out\n",
    "\n",
    "def slice1(x):\n",
    "    return x[:, :, :, 0:64]\n",
    "\n",
    "def slice2(x):\n",
    "    return x[:, :, :, 64:128]\n",
    "\n",
    "def slice1_output_shape(input_shape):\n",
    "    return tuple([input_shape[0],input_shape[1],input_shape[2],64])\n",
    "\n",
    "def slice2_output_shape(input_shape):\n",
    "    return tuple([input_shape[0],input_shape[1],input_shape[2],64])\n",
    "\n",
    "# Attention weighted sum\n",
    "def outfunc(vects):\n",
    "    cla, att = vects    # (N, n_time, n_out), (N, n_time, n_out)\n",
    "    att = K.clip(att, 1e-7, 1.)\n",
    "    out = K.sum(cla * att, axis=1) / K.sum(att, axis=1)     # (N, n_out)\n",
    "    return out\n",
    "\n",
    "def rnn(model,num_classes):\n",
    "    a1 = Reshape((n_time, n_freq, 1))(model) # (N, 99, 161, 1)\n",
    "    \n",
    "    a1 = block(a1)\n",
    "    a1 = block(a1)\n",
    "    a1 = MaxPooling2D(pool_size=(1, 2))(a1) # (N, 99, 81, 128)\n",
    "    \n",
    "    a1 = block(a1)\n",
    "    a1 = block(a1)\n",
    "    a1 = MaxPooling2D(pool_size=(1, 2))(a1) # (N, 99, 41, 128)\n",
    "    \n",
    "    a1 = block(a1)\n",
    "    a1 = block(a1)\n",
    "    a1 = MaxPooling2D(pool_size=(1, 2))(a1) # (N, 99, 21, 128)\n",
    "    \n",
    "    a1 = block(a1)\n",
    "    a1 = block(a1)\n",
    "    a1 = MaxPooling2D(pool_size=(1, 2))(a1) # (N, 99, 11, 128)\n",
    "    \n",
    "    a1 = block(a1)\n",
    "    a1 = block(a1)\n",
    "    a1 = MaxPooling2D(pool_size=(1, 2))(a1) # (N, 99, 5, 128)\n",
    "    \n",
    "    a1 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(a1)\n",
    "    a1 = MaxPooling2D(pool_size=(1, 5))(a1) # (N, 99, 1, 256)\n",
    "    \n",
    "    a1 = Reshape((99, 256))(a1) # (N, 99, 256)\n",
    "    \n",
    "    # Gated BGRU\n",
    "    rnnout = Bidirectional(GRU(128, activation='linear', return_sequences=True))(a1)\n",
    "    rnnout_gate = Bidirectional(GRU(128, activation='sigmoid', return_sequences=True))(a1)\n",
    "    a2 = Multiply()([rnnout, rnnout_gate])\n",
    "    \n",
    "    # Attention\n",
    "    cla = TimeDistributed(Dense(num_classes, activation='sigmoid'), name='localization_layer')(a2)\n",
    "    att = TimeDistributed(Dense(num_classes, activation='softmax'))(a2)\n",
    "    out = Lambda(outfunc, output_shape=(num_classes,))([cla, att])\n",
    "    return Model(input_logmel, out)\n",
    "\n",
    "input_logmel = Input(shape=(n_time, n_freq, 1), name='in_layer')   # (N, 99, 161)\n",
    "rnn_model = rnn(input_logmel, num_classes)\n",
    "rnn_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51088 samples, validate on 6798 samples\n",
      "Epoch 1/3\n",
      "51088/51088 [==============================] - 878s - loss: 0.1045 - acc: 0.9629 - val_loss: 0.0598 - val_acc: 0.9784\n",
      "Epoch 2/3\n",
      "51088/51088 [==============================] - 881s - loss: 0.0394 - acc: 0.9862 - val_loss: 0.0563 - val_acc: 0.9809\n",
      "Epoch 3/3\n",
      "39168/51088 [======================>.......] - ETA: 199s - loss: 0.0296 - acc: 0.9901"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=os.path.join(TEMP_DATA_PATH, 'rnn_model-checkpoint-unknown-{epoch:02d}-{val_loss:.2f}.hdf5'))\n",
    "rnn_model.fit(train_x_spectrum, train_y_one_hot, validation_data=(valid_x_spectrum, valid_y_one_hot), \n",
    "          batch_size=32, epochs=3, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save('rnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "for label, uid, path in TEST_FILES:\n",
    "    sample_rate, sample = wavfile.read(os.path.join(DATA_PATH, 'audio', path))\n",
    "    test_data.append((label, uid, sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_spectrum, test_y = shuffle_onehot_spectrum_transform(test_data, class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6835/6835 [==============================] - 24s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.032526497917976051, 0.9889605747493585]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.evaluate(x=test_x_spectrum, y=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** get Kaggle test score **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_id_matcher = re.compile('_([^\\.]+)\\.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_spectrum_transform(samples):\n",
    "    x = list(map(lambda t: t[1], samples))\n",
    "    x = list(map(pad_to_middle, x))\n",
    "    spectrum = np.stack(map(log_specgram, x))\n",
    "    spectrum = np.array(spectrum)\n",
    "    spectrum = spectrum.reshape(-1, 99, 161, 1)\n",
    "    \n",
    "    return spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-b55f773ade5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_spectrum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_spectrum_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-265c9859e598>\u001b[0m in \u001b[0;36mtest_spectrum_transform\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_to_middle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mspectrum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_specgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mspectrum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mspectrum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectrum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m161\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_spectrum = test_spectrum_transform(test_samples_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_spectrum_h5f = h5py.File('temp/test_spectrum.h5', 'w')\n",
    "test_spectrum_h5f.create_dataset('test_spectrum', data=test_spectrum)\n",
    "test_spectrum_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_samples_raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uids = np.array([item[0] for item in test_samples_raw]).astype('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uids_h5f = h5py.File('temp/uids.h5', 'w')\n",
    "uids_h5f.create_dataset('uids', data=uids)\n",
    "uids_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_spectrum = h5py.File('temp/test_spectrum.h5', 'r').get('test_spectrum')\n",
    "test_samples_raw = h5py.File('temp/test_samples_raw.h5', 'r').get('test_samples_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_model = load_model('silence_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silence_result = silence_model.predict(test_spectrum, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(silence_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_result_index = [np.argmax(t) for t in silence_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(silence_result_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_model_google= load_model('silence_model_google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_model_google_result = silence_model_google.predict(test_spectrum, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_model_google_result_index = [np.argmax(t) for t in silence_model_google_result]\n",
    "len(silence_model_google_result_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(silence_model_google_result_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_silence_model = load_model('mixed_silence_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_silence_model_result = mixed_silence_model.predict(test_spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_silence_model_result_index = list(map(np.argmax, mixed_silence_model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(mixed_silence_model_result_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_model = load_model('classes_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = classes_model.predict(test_spectrum, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_index = [ np.argmax(t) for t in result ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uids = [t[0] for t in test_samples_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_dict = {'unknown': 0, 'yes': 1, 'right': 2, 'stop': 3, 'up': 4, 'left': 5, 'down': 6, 'off': 7, 'go': 8, 'on': 9, 'no': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_reverse_dict = {}\n",
    "for key, value in class_dict.items():\n",
    "    class_reverse_dict[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_result = []\n",
    "\n",
    "for uid, index, silence_index in zip(uids, result_index, silence_result_index):\n",
    "    if silence_index == 1:\n",
    "        \n",
    "    \n",
    "    final_result.append((uid, class_reverse_dict[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['fname', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for uid, label in final_result:\n",
    "         writer.writerow({'fname': 'clip_' + uid + '.wav', 'label': label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# TensorFlow Speech Recognition Challenge (Directed Research)

Link: https://www.kaggle.com/c/tensorflow-speech-recognition-challenge

## 1. Description

> We might be on the verge of too many screens. It seems like everyday, new
versions of common objects are “re-invented” with built-in wifi and bright
touchscreens. A promising antidote to our screen addiction are voice interfaces.

Planet Aerial Imagery

But, for independent makers and entrepreneurs, it’s hard to build a simple speech
detector using free, open data and code. Many voice recognition datasets require
preprocessing before a neural network model can be built on them. To help with
this, TensorFlow recently released the Speech Commands Datasets. It includes
65,000 **one-second** long utterances of **30 short words**, by thousands of
different people.

In this competition, you're challenged to use the Speech Commands Dataset to
**build an algorithm that understands simple spoken commands**. By improving the
**recognition accuracy of open-sourced voice interface tools**, we can improve
product effectiveness and their accessibility.

## 2. Todo

- Read papers
  > Look at papers from the past few years on keyword spotting to learn what other
people have done.(Eg. Search for "keyword spotting speech" on scholar.google.com)
Find at least two good papers (eg. highly cited) and try to understand everything
in that paper.

  - "SMALL-FOOTPRINT KEYWORD SPOTTING USING DEEP NEURAL NETWORKS"  
    In this paper, they introduced a deep KWS system. It uses 13-dimensional PLP
    and $\Delta$ and $\Delta \Delta$ to detect the speech region and run
    40-dimentsional log-filter engrgies to generates feature for each frame. Then
    the system input the feature to a deep neural network to generate posterior
    for each key word
  - "SPEECH RECOGNITION AND KEYWORD SPOTTING FOR LOW RESOURCE"
    In this paper, they use DNN as the feature extractor and then input the data
    into other model, i.e Tandem model and the stacked hybird model. The aim to create
    a language independent acoustic model.  
  - "COMPARISON OF KEYWORD SPOTTING APPROACHES FOR INFORMAL CONTINUOUS SPEECH"
    It compares three KWS methods (i.e: acoustic keyword spotting, spotting in
    word lattice generated by LVCSR, hybird approach with lattices generated by
    phoneme recongnizer). Acoustic KWS uses crossword tri-phone models so it advantages
    for its simplicity. LVCSR KWS gives the best result however it has a drawback
    of strong dependence on dictionary words. Phoneme lattice KWS gives the worest
    result but it is able to combined both advantanges.
  - "Feature Extraction Methods LPC, PLP and MFCC"
    It explains what the Mel Frequency Cepstrum Coefficients and Perceptual Linear
    Prediction are. It is a kind of feature extraction method. Frame the signal
    and do FFT for each of them. Pass the fequency data into Mel filter bank and
    take logithm of them. Finally, use the iFFT to reverse them to get the Mel Cepstrum.
    PLP is another feature extraction method which is more adapted to human hearing.


- [-] TensorFlow Audio Recognition Tutorial

- [ ] Go through some good posts on Kaggle